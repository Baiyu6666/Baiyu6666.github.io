
<!doctype html>
<html lang="en">
  <head>
  <script src="https://use.fontawesome.com/baff6f55f5.js"></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Homepage of Baiyu Peng</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-29643011-3', 'auto');
      ga('send', 'pageview');
    </script>

    <!-- For all browsers -->
    <link rel="stylesheet" href="assets/css/academicons.min.css"/>
    <link rel="stylesheet" href="assets/css/academicons.css"/>
    
    <style>
      button.accordion {
      font:14px/1.5 Lato, "Helvetica Neue", Helvetica, Arial, sans-serif;
      cursor: pointer;
      padding: 0px;
      border: none;
      text-align: left;
      outline: none;
      font-size: 100%;
      transition: 0.3s;
      background-color: #f8f8f8;
      }
      button.accordion.active, button.accordion:hover {
      background-color: #f8f8f8;
      }
      button.accordion:after {
      content: " [+] ";
      font-size: 90%;
      color:#777;
      float: left;
      margin-left: 1px;
      }

      button.accordion.active:after {
      content: " [\2212] ";
      }
      div.panel {
      padding: 0 20px;
      margin-top: 5px;
      display: none;
      background-color: white;
      font-size: 100%;
      }
      div.panel.show {
      display: block !important;
      }
      .social-row {
        display: flex;
        flex-wrap: wrap;
        justify-content: space-between;
      }
    </style>
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Baiyu Peng</h1>
        <p>Master's student in Mechanical Engineering<br>Tsinghua University</p>
        <p>Bachelor's degree in Vehicle Engineering<br>Tsinghua University</p>
    <h3><a href="https://Baiyu6666.github.io/">Home</a></h3>
    <h3><a href="https://Baiyu6666.github.io/publication/CV.pdf">CV</a></h3>
    <h3><a href="https://Baiyu6666.github.io/research.html">Research</a></h3>
    <h3><a href="https://Baiyu6666.github.io/publication.html">Publication</a></h3>
    <h3><a href="https://Baiyu6666.github.io/personal.html">Daily life</a></h3>
    <b>Social</b><br>
        <div class="social-row">
<!--          <a href="Baiyu6666@gmail.com" class="author-social" target="_blank"><i class="fa fa-fw fa-envelope-square"></i> Email</a><br>-->
<!--          <a href=" https://scholar.google.com/citations?user=HK4x3fkAAAAJ&hl=zh-CN" target="_blank"><i class="ai ai-fw ai-google-scholar-square"></i> Scholar</a><br>-->
          <a href="https://www.researchgate.net/profile/Baiyu-Peng-2" class="author-social" target="_blank"><i class="fa fa-fw fa-researchgate-square"></i> Researchgate</a><br>
<!--          <a href="https://orcid.org/0000-0002-6910-0363"><i class="ai ai-fw ai-orcid-square"></i> ORCID</a><br>-->
<!--          <a href="https://github.com/Baiyu6666"><i class="fa fa-fw fa-github-square"></i> GitHub</a><br>-->
<!--          <a href="https://www.linkedin.com/in/mu-yao-308607185" class="author-social" target="_blank"><i class="fa fa-fw fa-linkedin-square"></i> LinkedIn</a><br>-->
          <br>
        </div>
        <br>

    <p><b>Contact:</b><br><i class="fa fa-fw fa-envelope-square"></i>Emails: pby19@mails.tsinghua.edu.cn<br><i class="fa fa-fw fa-phone-square"></i>Phone: +86 18810507500<br>
      </header>
      <section>

    <h2><a id="published-papers-updated" class="anchor" href="#publications" aria-hidden="true"><span class="octicon octicon-link"></span></a>Published &amp; Forthcoming Papers</h2>
      <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://Baiyu6666.github.io/publication/SPILjournal.pdf">Model-based Chance-Constrained Reinforcement Learning via Separated Proportional-Integral Lagrangian</a>
          <br> <b>Baiyu Peng</b>, Jingliang Duan, Jianyu Chen, Shengbo Eben Li, Genjin Xie, Congsheng Zhang, Yang Guan, Yao Mu, Enxin Sun <br>  <i>Submitted to IEEE Transactions on Neural Networks and Learning Systems, under review</i>. <a href="https://youtu.be/oVDB2XqNoCU">[video of real robot experiments]</a><br>
      <button class="accordion">
    Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> Safety is essential for reinforcement learning (RL) applied in the real world. Adding chance constraints (or probabilistic constraints) is a suitable way to enhance RL safety under uncertainty. Existing chance-constrained RL methods like the penalty methods and the Lagrangian methods either exhibit periodic oscillations or learn an over-conservative or unsafe policy.
In this paper, we address these shortcomings by proposing a separated proportional-integral Lagrangian (SPIL) algorithm. We first review the constrained policy optimization process from a feedback control perspective, which regards the penalty weight as the control input and the safe probability as the control output. Based on this, the penalty method is formulated as a proportional controller, and the Lagrangian method is formulated as an integral controller. We then unify them and present a proportional-integral Lagrangian method to get both their merits, with an integral separation technique to limit the integral value in a reasonable range. To accelerate training, the gradient of safe probability is computed in a model-based manner. We demonstrate our method can reduce the oscillations and conservatism of RL policy in a car-following simulation. To prove its practicality, we also apply our method to a real-world mobile robot navigation task, where our robot successfully avoids a moving obstacle with highly uncertain or even aggressive behaviors.</p></div>




          <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://Baiyu6666.github.io/publication/SPIL.pdf">Separated Proportional-Integral Lagrangian for Chance Constrained Reinforcement Learning</a>
          <br> <b>Baiyu Peng</b>, Yao Mu, Jingliang Duan, Yang Guan, Shengbo Eben Li, Jianyu Chen <br>  <i>2021th IEEE Intelligent Vehicle Symposium (IV)</i>, <b>Finalist for Student Best Paper Award, Top1%, 3/220</b>. <a href="https://youtu.be/lsOE4nWvjoA">[video]</a><br>
      <button class="accordion">
    Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> Safety is essential for reinforcement learning (RL) applied in real-world tasks like autonomous driving. Imposing chance constraints (or probabilistic constraints) is a suitable way to enhance RL safety under model uncertainty. Existing chance constrained RL methods like the penalty methods and the Lagrangian methods either exhibit periodic oscillations or learn an over-conservative or unsafe policy.
In this paper, we address these shortcomings by elegantly combining these two methods and propose a separated proportional-integral Lagrangian (SPIL) algorithm. We first rewrite penalty methods as optimizing safe probability according to the proportional value of constraint violation, and Lagrangian methods as optimizing according to the integral value of the violation. Then we propose to add up both the integral and proportion values to optimize the policy, with an integral separation technique to limit the integral value within a reasonable range. Besides, the gradient of policy is computed in a model-based paradigm to accelerate training. The proposed method is proved to reduce oscillations and conservatism while ensuring safety by a car-following experiment. </p></div>



          <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://Baiyu6666.github.io/publication/CCAC.pdf">Model-Based Actor-Critic with Chance Constraint for Stochastic System</a> <br>  <b>Baiyu Peng</b>, Yao Mu, Yang Guan, Shengbo Eben Li, Yuming Yin, Jianyu Chen <br> <i>2021 IEEE 60th Conference on Decision and Control (CDC)</i>. <a href="https://youtu.be/B5tIsIDLMDA">[video]</a> <br>
    <button class="accordion">
        Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> Safety is essential for reinforcement learning (RL) applied in real-world situations. Chance constraints are suitable to represent the safety requirements in stochastic systems. Previous chance constrained RL methods usually learn an either conservative or unsafe policy, and some of them also suffer from a low convergence rate. In this paper, we propose a model-based chance constrained actor-critic (CCAC) algorithm which can efficiently learn a safe and non-conservative policy. Different from existing methods that optimize a conservative lower bound, CCAC directly solves the original chance constrained problems, where the objective function and safe probability are simultaneously optimized with adaptive weights. In order to improve the convergence rate, CCAC utilizes the gradient of dynamic model to accelerate policy optimization. The effectiveness of CCAC is demonstrated by a stochastic car-following task. Experiments indicate that CCAC achieves good performance while guaranteeing safety, with a five times faster convergence rate compared with model-free RL methods. It also has 100 times higher online computation efficiency than traditional safety techniques such as stochastic model predictive control.   </p></div>

          <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://Baiyu6666.github.io/publication/MixedEPO.pdf">Mixed Reinforcement Learning for Efficient Policy Optimization in Stochastic Environments</a>  <br> Yao Mu, <b>Baiyu Peng</b>, Ziqing Gu, Shengbo Eben Li, Chang Liu, Bingbing Nie, Jianfeng Zheng, Bo Zhang <br> <i>2020 20th International Conference on Control, Automation and Systems (ICCAS)</i>, <b>Student Best Paper Award, Top1%, 5/500</b>. <br><button class="accordion">
    Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> Reinforcement learning has the potential to control stochastic nonlinear systems in optimal manners successfully. We propose a mixed reinforcement learning (mixed RL) algorithm by simultaneously using dual representations of environmental dynamics to search the optimal policy. The dual representation includes an empirical dynamic model and a set of state-action data. The former can embed the designer's knowledge and reduce the difficulty of learning, and the latter can be used to compensate the model inaccuracy since it reflects the real system dynamics accurately. Such a design has the capability of improving both learning accuracy and training speed. In the mixed RL framework, the additive uncertainty of stochastic model is compensated by using explored state-action data via iterative Bayesian estimator (IBE). The optimal policy is then computed in an iterative way by alternating between policy evaluation (PEV) and policy improvement (PIM). The effectiveness of mixed RL is demonstrated by a typical optimal control problem of stochastic non-affine nonlinear systems (i.e., double lane change task with an automated vehicle). </p></div>

    <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://Baiyu6666.github.io/publication/DDDQN.pdf">End-to-End Autonomous Driving through Dueling Double Deep Q-Network</a>  <br> <b>Baiyu Peng</b>, Qi Sun, Shengbo Eben Li, Dongsuk Kum, Yuming Yin, Junqing Wei, Tianyu Gu <br> <i>Automotive Innovation 4(3), 328–337</i>. <br><button class="accordion">
    Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> Recent years have seen the rapid development of autonomous driving systems, which are typically designed in a hierarchical architecture or an end-to-end architecture. The hierarchical architecture is always complicated and hard to design, while the end-to-end architecture is more promising due to its simple structure. This paper puts forward an end-to-end autonomous driving method through a deep reinforcement learning algorithm Dueling Double Deep Q-Network, making it possible for the vehicle to learn end-to-end driving by itself. This paper firstly proposes an architecture for the end-to-end lane-keeping task. Unlike the traditional image-only state space, the presented state space is composed of both camera images and vehicle motion information. Then corresponding dueling neural network structure is introduced, which reduces the variance and improves sampling efficiency. Thirdly, the proposed method is applied to The Open Racing Car Simulator (TORCS) to demonstrate its great performance, where it surpasses human drivers. Finally, the saliency map of the neural network is visualized, which indicates the trained network drives by observing the lane lines. A video for the presented work is available online, https://youtu.be/76ciJmIHMD8 or https://v.youku.com/v_show/id_XNDM4ODc0MTM4NA==.html</p></div>



    <script src="javascripts/scale.fix.js"></script>
    <script> 
    var acc = document.getElementsByClassName("accordion");
    var i;

    for (i = 0; i < acc.length; i++) {
        acc[i].onclick = function(){
            this.classList.toggle("active");
            this.parentNode.nextElementSibling.classList.toggle("show");
      }
    }
    </script>
  </body>
</html>
